# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
"""
Jinja2 templates for Slurm cluster configuration scaffolding.

These templates are used by `hyp init slurm-cluster` to generate the initial
configuration files with sensible defaults and helpful comments.
"""

# config.yaml template for Slurm cluster configuration
CONFIG_TEMPLATE_CONTENT = """# SageMaker HyperPod Slurm Cluster Configuration
# Generated by: hyp init slurm-cluster
# Documentation: https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod.html

# Schema version - do not modify
version: "1.0"
template: slurm-cluster

# Cluster name (1-63 characters, alphanumeric and hyphens only)
cluster_name: {{ cluster_name | default('my-slurm-cluster') }}

# AWS region (optional - uses default region if not specified)
{% if region %}
region: {{ region }}
{% else %}
# region: us-west-2
{% endif %}

# Instance Groups Configuration
# Define the compute resources for your cluster
instance_groups:
  # Controller node - manages the Slurm scheduler
  - instance_group_name: controller
    instance_type: {{ controller_instance_type | default('ml.t3.medium') }}
    instance_count: 1
    execution_role: {{ execution_role | default('arn:aws:iam::123456789012:role/SageMakerExecutionRole') }}
    lifecycle_config:
      source_s3_uri: {{ lifecycle_s3_uri | default('s3://your-bucket/lifecycle-scripts') }}
      on_create: on_create.sh
    instance_storage_configs:
      - ebs_volume_config:
          volume_size_in_gb: {{ controller_volume_size | default(500) }}

  # Worker nodes - run your training jobs
  - instance_group_name: worker
    instance_type: {{ worker_instance_type | default('ml.p4d.24xlarge') }}
    instance_count: {{ worker_instance_count | default(2) }}
    execution_role: {{ execution_role | default('arn:aws:iam::123456789012:role/SageMakerExecutionRole') }}
    lifecycle_config:
      source_s3_uri: {{ lifecycle_s3_uri | default('s3://your-bucket/lifecycle-scripts') }}
      on_create: on_create.sh
    instance_storage_configs:
      - ebs_volume_config:
          volume_size_in_gb: {{ worker_volume_size | default(500) }}
{% if threads_per_core %}
    threads_per_core: {{ threads_per_core }}
{% endif %}

# VPC Configuration
# Specify the networking configuration for your cluster
vpc_config:
  subnets:
{% for subnet in subnets | default(['subnet-xxxxxxxxxxxxxxxxx']) %}
    - {{ subnet }}
{% endfor %}
  security_group_ids:
{% for sg in security_group_ids | default(['sg-xxxxxxxxxxxxxxxxx']) %}
    - {{ sg }}
{% endfor %}

# Node Recovery Setting
# Automatic: HyperPod automatically replaces unhealthy nodes
# None: Manual intervention required for node recovery
node_recovery: {{ node_recovery | default('Automatic') }}

# Resource Tags (optional)
# Add tags to help organize and identify your cluster resources
{% if tags %}
tags:
{% for tag in tags %}
  - key: {{ tag.key }}
    value: {{ tag.value }}
{% endfor %}
{% else %}
# tags:
#   - key: Environment
#     value: Development
#   - key: Project
#     value: MLTraining
{% endif %}

# Slurm-specific Configuration
slurm_config:
  # Controller group - must match an instance_group_name above
  controller_group: controller

  # Login group (optional) - for SSH access to the cluster
{% if login_group %}
  login_group: {{ login_group }}
{% else %}
  # login_group: login
{% endif %}

  # Worker groups - map instance groups to Slurm partitions
  worker_groups:
    - instance_group_name: worker
      partition_name: {{ partition_name | default('dev') }}

  # FSx for Lustre shared storage (optional)
{% if fsx_dns_name %}
  fsx_dns_name: {{ fsx_dns_name }}
  fsx_mountname: {{ fsx_mountname }}
{% else %}
  # fsx_dns_name: fs-xxxxxxxxxxxxxxxxx.fsx.us-west-2.amazonaws.com
  # fsx_mountname: xxxxxxxx
{% endif %}
"""

# provisioning_parameters.json template for Slurm-specific settings
PROVISIONING_PARAMS_TEMPLATE_CONTENT = """{
  "version": "1.0",
  "workload_manager": "slurm",
  "controller_group": "{{ controller_group | default('controller') }}",
  "worker_groups": [
    {
      "instance_group_name": "{{ worker_group_name | default('worker') }}",
      "partition_name": "{{ partition_name | default('dev') }}"
    }
  ],
  "fsx_dns_name": {{ fsx_dns_name | default('null') | tojson }},
  "fsx_mountname": {{ fsx_mountname | default('null') | tojson }}
}
"""
